# General Libs/Required libs
import pandas as pd
import json
from io import StringIO
import datetime as dt
import pytz
import pickle
import MLDB as ml
import logging

# S3 libs
import boto3
from botocore.exceptions import ClientError
import os
import sys

"""
load.py
- retrieves the JSON generated by ingest.py
Splits the file into two dictionaries of dataframes:
g_dict - has data for users that have models
b_dict - has users that need to go through training

The g_dict will be saved to users_with_models.pkl
The b_dict will be saved to users_with_no_models.pkl
"""

# s3 connection parameters:
aws_access_key_id = os.environ.get("AWS_ACCESS_KEY_ID")
aws_secret_access_key = os.environ.get("AWS_SECRET_ACCESS_KEY")
endpoint_url = os.environ.get("AWS_S3_ENDPOINT")
region_name = os.environ.get("AWS_DEFAULT_REGION")
bucket_name = os.environ.get("AWS_S3_BUCKET")


s3_client = boto3.client(
    "s3",
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key,
    endpoint_url=endpoint_url,
    region_name=region_name,
)


def read_json(s3):
    """
    Reads a json file from the s3 bucket data folder using s3_client,
    then loads as DataFrame format
    """

    timestamp = dt.datetime.now(pytz.timezone("America/New_York")).strftime("%m%d%y_%H")
    file_key = f"data/splunkdata{timestamp}.json"
    try:
        response = s3.get_object(Bucket=bucket_name, Key=file_key)
        # Read the content
        content = response["Body"].read()
        # Parse JSON
        json_object = json.loads(content)
        data = pd.read_json(StringIO(json.dumps(json_object)))
        return data

    except ClientError as e:
        logging.error(e)
        df = pd.DataFrame()
        # return empty dataframe
        return df


def process_list(data, G_List):
    """
    process_list()
    Takes the complete set in df format and creates dictionaries for users
    with and without models

    Returns:
        - g_dict: Dictionary with users with models as keys and their data as values
        - b_dict: Dictionary with users without models as keys and their data as values
    """
    # Create dictionaries for both groups
    g_dict, b_dict = dict(), dict()

    # with users that have models
    for user in G_List:
        if user in data["Account_Name"].values:
            g_dict[user] = data[data["Account_Name"] == user].reset_index(drop=True)

    # with users that don't have models
    b_users = data[~data["Account_Name"].isin(G_List)]["Account_Name"].unique()
    for user in b_users:
        b_dict[user] = data[data["Account_Name"] == user].reset_index(drop=True)

    return g_dict, b_dict


def write_pickle(data, pickle_name, s3):
    # Pickle the data
    pickle_data = pickle.dumps(data)
    timestamp = dt.datetime.now(pytz.timezone("America/New_York")).strftime("%m%d%y_%H")
    file_key = f"pickles/{timestamp}/{pickle_name}.pickle"
    # Upload the pickle data to S3
    try:
        s3.put_object(Bucket=bucket_name, Key=file_key, Body=pickle_data)
    except ClientError as e:
        logging.error(e)
        return False
    return True


if __name__ == "__main__":
    try:
        ret, Good_List, msg = ml.get_models("G")
        if ret == 0:
            err_msg = "Problem retrieving database info" + msg
            ml.write_log(err_msg, 3, "Load.py", ml.lineno())
            sys.exit(3)
        else:
            g_msg = f"{len(Good_List)} users found with models"
            ml.write_log(g_msg, 1, "Load.py", "")
        df = read_json(s3_client)
        if df.empty:
            err_msg = "No data retrieved from s3 bucket"
            ml.write_log(err_msg, 3, "Load.py", ml.lineno())
            sys.exit(3)
        g_dict, b_dict = process_list(df, Good_List)
        if not g_dict:
            g_msg = "No users with models data found"
            ml.write_log(g_msg, 1, "Load.py", "")
        else:
            write_pickle(g_dict, "users_with_models", s3_client)
            print("Users with Models were saved in the bucket")
        if not b_dict:
            b_msg = "No users without models data found"
            ml.write_log(b_msg, 1, "Load.py", "")
        else:
            write_pickle(b_dict, "users_with_no_models", s3_client)
            print("Users without Models were saved in the bucket")
        ml.write_log("Load successful", 1, "Load.py", "")
    except Exception as e:
        err_msg = f"Problem: {format(e)} occurred {ml.lineno()}"
        ml.write_log(err_msg, 2, "Load.py", "")
        sys.exit(2)
