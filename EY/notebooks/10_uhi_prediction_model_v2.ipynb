{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from pycaret.regression import *\n",
    "from shapely.geometry import Point, buffer\n",
    "import geopandas as gpd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the satellite data\n",
    "def load_geotiff(file_path):\n",
    "    with rasterio.open(file_path) as src:\n",
    "        return src.read(1), src.transform\n",
    "\n",
    "# Load Landsat LST data\n",
    "lst_data, lst_transform = load_geotiff('Landsat_LST.tiff')\n",
    "\n",
    "# Load Sentinel-2 data (contains NDVI, NDWI, EVI)\n",
    "s2_data, s2_transform = load_geotiff('S2_sample.tiff')\n",
    "\n",
    "train_df = pd.read_csv('Training_data_uhi_index_UHI2025-v2.csv')\n",
    "\n",
    "# Function to get pixel values at given coordinates\n",
    "def get_pixel_values(lat, lon, data, transform):\n",
    "    row, col = rasterio.transform.rowcol(transform, lon, lat)\n",
    "    try:\n",
    "        return data[row, col]\n",
    "    except IndexError:\n",
    "        return np.nan\n",
    "\n",
    "# Extract features for each training point\n",
    "train_df['lst'] = train_df.apply(lambda x: get_pixel_values(x['Latitude'], x['Longitude'], lst_data, lst_transform), axis=1)\n",
    "train_df['s2_features'] = train_df.apply(lambda x: get_pixel_values(x['Latitude'], x['Longitude'], s2_data, s2_transform), axis=1)\n",
    "\n",
    "# Drop rows with missing values\n",
    "train_df = train_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "def load_all_data():\n",
    "    # Original data\n",
    "    train_data = train_df.copy()\n",
    "    \n",
    "    # Load weather data\n",
    "    weather_data = pd.read_excel('NY_Mesonet_Weather.xlsx', sheet_name=['Manhattan', 'Bronx'])\n",
    "    manhattan_weather = weather_data['Manhattan']\n",
    "    bronx_weather = weather_data['Bronx']\n",
    "    \n",
    "    # Load building footprint\n",
    "    building_data = gpd.read_file('Building_Footprint.kml')\n",
    "    \n",
    "    return train_data, manhattan_weather, bronx_weather, building_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess weather data\n",
    "def process_weather_data(manhattan_weather, bronx_weather):\n",
    "    # First, let's print the data structure\n",
    "    print(\"Manhattan columns:\", manhattan_weather.columns)\n",
    "    print(\"Bronx columns:\", bronx_weather.columns)\n",
    "    \n",
    "    # Create copies to avoid modifying original data\n",
    "    manhattan_df = manhattan_weather.copy()\n",
    "    bronx_df = bronx_weather.copy()\n",
    "    \n",
    "    # Add location column\n",
    "    manhattan_df.loc[:, 'location'] = 'Manhattan'\n",
    "    bronx_df.loc[:, 'location'] = 'Bronx'\n",
    "    \n",
    "    # Combine the data\n",
    "    weather_combined = pd.concat([manhattan_df, bronx_df], ignore_index=True)\n",
    "    \n",
    "    # Verify the combined data\n",
    "    print(\"\\nCombined data columns:\", weather_combined.columns)\n",
    "    print(\"Combined data shape:\", weather_combined.shape)\n",
    "    \n",
    "    try:\n",
    "        # Group and aggregate\n",
    "        weather_features = weather_combined.groupby('location').agg({\n",
    "            'Air Temp at Surface [degC]': ['mean', 'max', 'min', 'std'],\n",
    "            'Relative Humidity [percent]': ['mean', 'max', 'min'],\n",
    "            'Avg Wind Speed [m/s]': 'mean',\n",
    "            'Solar Flux [W/m^2]': ['mean', 'max'],\n",
    "            'Wind Direction [degrees]': ['mean']\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Flatten and rename columns\n",
    "        weather_features.columns = [\n",
    "            'location',\n",
    "            'temp_mean', 'temp_max', 'temp_min', 'temp_std',\n",
    "            'humidity_mean', 'humidity_max', 'humidity_min',\n",
    "            'wind_speed_mean',\n",
    "            'solar_flux_mean', 'solar_flux_max',\n",
    "            'wind_direction_mean'\n",
    "        ]\n",
    "        \n",
    "        return weather_features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {str(e)}\")\n",
    "        print(\"\\nFirst few rows of combined data:\")\n",
    "        print(weather_combined.head())\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_location_column(df):\n",
    "    \"\"\"Add location column based on coordinates\"\"\"\n",
    "    # Define Manhattan and Bronx boundaries (approximate)\n",
    "    manhattan_bounds = {\n",
    "        'lat_min': 40.7,\n",
    "        'lat_max': 40.88,\n",
    "        'lon_min': -74.02,\n",
    "        'lon_max': -73.91\n",
    "    }\n",
    "    \n",
    "    bronx_bounds = {\n",
    "        'lat_min': 40.785,\n",
    "        'lat_max': 40.92,\n",
    "        'lon_min': -73.93,\n",
    "        'lon_max': -73.765\n",
    "    }\n",
    "    \n",
    "    def get_location(row):\n",
    "        lat = row['Latitude']\n",
    "        lon = row['Longitude']\n",
    "        \n",
    "        if (manhattan_bounds['lat_min'] <= lat <= manhattan_bounds['lat_max'] and \n",
    "            manhattan_bounds['lon_min'] <= lon <= manhattan_bounds['lon_max']):\n",
    "            return 'Manhattan'\n",
    "        elif (bronx_bounds['lat_min'] <= lat <= bronx_bounds['lat_max'] and \n",
    "              bronx_bounds['lon_min'] <= lon <= bronx_bounds['lon_max']):\n",
    "            return 'Bronx'\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "    \n",
    "    df['location'] = df.apply(get_location, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Data\n",
    "def calculate_density(building_data, train_data, radius=500):\n",
    "    \"\"\"Calculate building density within a radius around each point\"\"\"\n",
    "    # Create GeoDataFrame from training points\n",
    "    train_points = gpd.GeoDataFrame(\n",
    "        train_data, \n",
    "        geometry=[Point(xy) for xy in zip(train_data['Longitude'], train_data['Latitude'])]\n",
    "    )\n",
    "    \n",
    "    # Calculate density for each point\n",
    "    densities = []\n",
    "    for _, point in train_points.iterrows():\n",
    "        # Create buffer around point using the buffer method\n",
    "        buffer_zone = point.geometry.buffer(radius/111000)  \n",
    "        \n",
    "        # Count buildings within buffer\n",
    "        buildings_in_zone = building_data[building_data.geometry.intersects(buffer_zone)]\n",
    "        density = len(buildings_in_zone) / (np.pi * (radius/1000)**2)  # buildings per kmÂ²\n",
    "        densities.append(density)\n",
    "    \n",
    "    return densities\n",
    "\n",
    "def calculate_height(building_data, train_data, radius=500):\n",
    "    \"\"\"Calculate average building height within radius\"\"\"\n",
    "    from shapely.geometry import Point\n",
    "    \n",
    "    # Print available columns\n",
    "    print(\"Available columns in building data:\", building_data.columns.tolist())\n",
    "    \n",
    "    # Try different possible height column names\n",
    "    possible_height_columns = ['HEIGHT', 'height', 'Height', 'building_height', \n",
    "                             'BUILDING_HEIGHT', 'HeightRoof', 'HEIGHT_ROOF']\n",
    "    \n",
    "    height_col = None\n",
    "    for col in possible_height_columns:\n",
    "        if col in building_data.columns:\n",
    "            height_col = col\n",
    "            break\n",
    "    \n",
    "    if height_col is None:\n",
    "        print(\"Warning: No height column found. Using default height of 10 meters.\")\n",
    "        building_data['height'] = 10  # Set default height\n",
    "        height_col = 'height'\n",
    "    \n",
    "    # Create GeoDataFrame from training points\n",
    "    train_points = gpd.GeoDataFrame(\n",
    "        train_data, \n",
    "        geometry=[Point(xy) for xy in zip(train_data['Longitude'], train_data['Latitude'])]\n",
    "    )\n",
    "    \n",
    "    # Calculate average height for each point\n",
    "    avg_heights = []\n",
    "    for _, point in train_points.iterrows():\n",
    "        buffer_zone = point.geometry.buffer(radius/111000)\n",
    "        buildings_in_zone = building_data[building_data.geometry.intersects(buffer_zone)]\n",
    "        \n",
    "        if len(buildings_in_zone) > 0:\n",
    "            avg_height = buildings_in_zone[height_col].mean()\n",
    "        else:\n",
    "            avg_height = 0\n",
    "            \n",
    "        avg_heights.append(avg_height)\n",
    "    \n",
    "    return avg_heights\n",
    "\n",
    "def calculate_coverage(building_data, train_data, radius=500):\n",
    "    \"\"\"Calculate building coverage ratio within radius\"\"\"\n",
    "    from shapely.geometry import Point, Polygon  # Import Polygon\n",
    "\n",
    "    # Create GeoDataFrame from training points\n",
    "    train_points = gpd.GeoDataFrame(\n",
    "        train_data, \n",
    "        geometry=[Point(xy) for xy in zip(train_data['Longitude'], train_data['Latitude'])]\n",
    "    )\n",
    "    \n",
    "    # Calculate coverage for each point\n",
    "    coverage_ratios = []\n",
    "    for _, point in train_points.iterrows():\n",
    "        buffer_zone = point.geometry.buffer(radius/111000)\n",
    "        buildings_in_zone = building_data[building_data.geometry.intersects(buffer_zone)]\n",
    "        \n",
    "        if len(buildings_in_zone) > 0:\n",
    "            # Calculate total building footprint area\n",
    "            building_area = buildings_in_zone.geometry.area.sum()\n",
    "            # Calculate buffer zone area\n",
    "            buffer_area = buffer_zone.area\n",
    "            # Calculate coverage ratio\n",
    "            coverage = building_area / buffer_area\n",
    "        else:\n",
    "            coverage = 0\n",
    "            \n",
    "        coverage_ratios.append(coverage)\n",
    "    \n",
    "    return coverage_ratios\n",
    "\n",
    "def engineer_building_features(building_data, train_data):\n",
    "    \"\"\"Main function to calculate all building features\"\"\"\n",
    "    # Create empty DataFrame for features\n",
    "    building_features = pd.DataFrame()\n",
    "    \n",
    "    print(\"Calculating building density...\")\n",
    "    building_features['building_density'] = calculate_density(building_data, train_data)\n",
    "    \n",
    "    print(\"Calculating average building height...\")\n",
    "    building_features['avg_building_height'] = calculate_height(building_data, train_data)\n",
    "    \n",
    "    print(\"Calculating building coverage...\")\n",
    "    building_features['building_coverage'] = calculate_coverage(building_data, train_data)\n",
    "    \n",
    "    return building_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_matrix(train_data, weather_features, building_features):\n",
    "    # Combine all features\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "\n",
    "    # Add original features (excluding lat/lon)\n",
    "    original_features = train_data.drop(['Longitude', 'Latitude', 'UHI Index'], axis=1)\n",
    "    features = pd.concat([features, original_features], axis=1)\n",
    "    \n",
    "    # Add weather features\n",
    "    features = features.merge(weather_features, on='location', how='left')\n",
    "    \n",
    "    # Add building features\n",
    "    features = pd.concat([features, building_features], axis=1)\n",
    "    \n",
    "    # Add satellite features\n",
    "    features['lst'] = train_data['lst']\n",
    "    features['s2_features'] = train_data['s2_features'] \n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling\n",
    "def train_enhanced_model(features, target):\n",
    "    # Setup with enhanced parameters\n",
    "    setup(\n",
    "        data=features,\n",
    "        target=target,\n",
    "        session_id=123,\n",
    "        transformation=True,\n",
    "        polynomial_features=False,\n",
    "        feature_selection=True,\n",
    "        remove_multicollinearity=True,\n",
    "        fold=10\n",
    "    )\n",
    "    \n",
    "    # Create and tune multiple models\n",
    "    rf = create_model('rf')\n",
    "    xgb = create_model('xgboost')\n",
    "    lgb = create_model('lightgbm')\n",
    "    \n",
    "    # Tune each model\n",
    "    tuned_models = []\n",
    "    for model in [rf, xgb, lgb]:\n",
    "        tuned = tune_model(model, optimize='R2')\n",
    "        tuned_models.append(tuned)\n",
    "    \n",
    "    # Create stacked model\n",
    "    stacked = stack_models(tuned_models, meta_model=rf, optimize='R2')\n",
    "    \n",
    "    # Final tuning of stacked model\n",
    "    final_model = tune_model(stacked, optimize='R2')\n",
    "    \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "def predict_uhi(model, submission_data, weather_features, building_features):\n",
    "    # Prepare submission features\n",
    "    submission_features = create_feature_matrix(\n",
    "        submission_data,\n",
    "        weather_features,\n",
    "        building_features,\n",
    "        lst_data,\n",
    "        s2_data\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = predict_model(model, data=submission_features)\n",
    "    return predictions['prediction_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "# Load all required data\n",
    "weather_data = pd.read_excel('NY_Mesonet_Weather.xlsx', sheet_name=['Manhattan', 'Bronx'])\n",
    "building_data = gpd.read_file('Building_Footprint.kml')  # Add this line\n",
    "\n",
    "manhattan_weather = weather_data['Manhattan']\n",
    "bronx_weather = weather_data['Bronx']\n",
    "\n",
    "print(\"\\nData loaded successfully\")\n",
    "print(f\"Manhattan shape: {manhattan_weather.shape}\")\n",
    "print(f\"Bronx shape: {bronx_weather.shape}\")\n",
    "print(f\"Building data shape: {building_data.shape}\")\n",
    "\n",
    "# Process weather data\n",
    "weather_features = process_weather_data(manhattan_weather, bronx_weather)\n",
    "print(\"\\nWeather features created successfully\")\n",
    "print(weather_features.head())\n",
    "\n",
    "# Process building data\n",
    "building_features = engineer_building_features(building_data, train_df)\n",
    "print(\"\\nBuilding features created successfully\")\n",
    "print(building_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix\n",
    "features = create_feature_matrix(\n",
    "  add_location_column(train_df),\n",
    "  weather_features,\n",
    "  building_features,\n",
    "  lst_data,\n",
    "  s2_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"\\nTraining model...\")\n",
    "model = train_enhanced_model(features, train_df['UHI Index'])\n",
    "print(\"Model training completed\")\n",
    "\n",
    "# Make predictions on submission data\n",
    "print(\"\\nMaking predictions...\")\n",
    "submission_data = pd.read_csv('Submission_template_UHI2025-v2.csv')\n",
    "submission_building_features = engineer_building_features(building_data, submission_data)\n",
    "predictions = predict_uhi(\n",
    "  model, \n",
    "  submission_data, \n",
    "  weather_features, \n",
    "  submission_building_features\n",
    ")\n",
    "\n",
    "# Save predictions\n",
    "submission_data['UHI Index'] = predictions\n",
    "submission_data[['Longitude','Latitude','UHI Index']].to_csv('submission_predictions.csv', index=False)\n",
    "print(\"\\nPredictions saved to 'UHI submission additional data.csv'\")\n",
    "\n",
    "# Print model performance metrics\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(pull())\n",
    "\n",
    "# Plot feature importance\n",
    "plot_model(model, plot='feature')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
